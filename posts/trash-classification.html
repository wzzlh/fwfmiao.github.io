<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="msvalidate.01" content="B3C8AB35B1F97A3EBC4244FFFA656AE9">
  <meta name="yandex-verification" content="DT5rdVpWuLkbg2sACQp0zsF3Fv8wTb-HPYijYJprwD8">
  <meta name="baidu-site-verification" content="yBSgXL4Z57vA68tQ">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"liufengyu.cn","root":"/","scheme":"Gemini","version":"7.8.0","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":true,"nav":{"disqus":{"text":"Load Disqus","order":-1}},"activeClass":"disqus"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="前言目前我国垃圾分类存在的主要问题有三点： 1，垃圾分类正确率不高。 2，居民缺乏垃圾分类的意识和相关知识。 3，没有真正意义上的高效的垃圾分类系统。 基于以上，我们用深度学习的方法做垃圾分类。从技术上旨在通过深度学习，实现垃圾的高精确度分类。 神经网络神经网络简介人工神经网络（ Artificial Neural Network， 简写为ANN）也简称为神经网络（NN），是一种模仿生物神经网络结">
<meta property="og:type" content="article">
<meta property="og:title" content="【深度学习】手把手教你用卷积神经网络做垃圾分类">
<meta property="og:url" content="https://liufengyu.cn/posts/trash-classification.html">
<meta property="og:site_name" content="飞鱼塘">
<meta property="og:description" content="前言目前我国垃圾分类存在的主要问题有三点： 1，垃圾分类正确率不高。 2，居民缺乏垃圾分类的意识和相关知识。 3，没有真正意义上的高效的垃圾分类系统。 基于以上，我们用深度学习的方法做垃圾分类。从技术上旨在通过深度学习，实现垃圾的高精确度分类。 神经网络神经网络简介人工神经网络（ Artificial Neural Network， 简写为ANN）也简称为神经网络（NN），是一种模仿生物神经网络结">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E5%88%86%E7%B1%BB.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/softmax%E5%9B%9E%E5%BD%92.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/softmax%E5%B1%95%E5%BC%80.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E4%BA%A4%E5%8F%89%E6%8D%9F%E5%A4%B1%E7%90%86%E8%A7%A3.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/image-20190818150116295.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%9B%BE.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%90%86%E8%A7%A3.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%95%B0%E5%AD%97.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%95%B0%E5%AD%97%E5%BC%A0%E9%87%8F.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%95%B0%E5%AD%97label.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%84%E6%A6%82%E8%BF%B0.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E5%8D%B7%E7%A7%AF%E5%B1%82.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E8%AE%A1%E7%AE%97%E5%9B%BE1.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E8%AE%A1%E7%AE%97%E5%9B%BE2.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E8%AE%A1%E7%AE%97%E5%9B%BE3.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E8%AE%A1%E7%AE%97%E5%8A%A8%E5%9B%BE.gif">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%AD%A5%E9%95%BF%E4%B8%BA2.1.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%AD%A5%E9%95%BF%E4%B8%BA2.2.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%AD%A5%E9%95%BF%E4%B8%BA2.3.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E6%AD%A5%E9%95%BF%E4%B8%BA2.4.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/%E5%A4%9A%E4%B8%AAFilter.gif">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/Relu.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/Pooling.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/1527656989598.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/1527656989203.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/ResNet.png">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/90.jpeg">
<meta property="og:image" content="https://liufengyu.cn/posts/trash-classification/glass108.jpg">
<meta property="article:published_time" content="2019-09-01T07:59:25.000Z">
<meta property="article:modified_time" content="2020-08-09T08:05:48.996Z">
<meta property="article:author" content="飞木鱼">
<meta property="article:tag" content="Tensorflow">
<meta property="article:tag" content="Keras">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://liufengyu.cn/posts/trash-classification/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png">

<link rel="canonical" href="https://liufengyu.cn/posts/trash-classification.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【深度学习】手把手教你用卷积神经网络做垃圾分类 | 飞鱼塘</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">飞鱼塘</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">飞木鱼的个人博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">16</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">1</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">13</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://liufengyu.cn/posts/trash-classification.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="飞木鱼">
      <meta itemprop="description" content="整理一些值得留意的文章">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="飞鱼塘">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【深度学习】手把手教你用卷积神经网络做垃圾分类
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-01 15:59:25" itemprop="dateCreated datePublished" datetime="2019-09-01T15:59:25+08:00">2019-09-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-09 16:05:48" itemprop="dateModified" datetime="2020-08-09T16:05:48+08:00">2020-08-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%99%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">教程</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/posts/trash-classification.html#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="/posts/trash-classification.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前我国垃圾分类存在的主要问题有三点：</p>
<p>1，垃圾分类正确率不高。</p>
<p>2，居民缺乏垃圾分类的意识和相关知识。</p>
<p>3，没有真正意义上的高效的垃圾分类系统。</p>
<p>基于以上，我们用深度学习的方法做垃圾分类。从技术上旨在通过深度学习，实现垃圾的高精确度分类。</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="神经网络简介"><a href="#神经网络简介" class="headerlink" title="神经网络简介"></a>神经网络简介</h3><p>人工神经网络（ Artificial Neural Network， 简写为ANN）也简称为神经网络（NN），是一种模仿生物神经网络结构和功能的计算模型。经典的神经网络结构包含三个层次的神经网络。分别为输入层，输出层以及隐藏层。</p>
<p><img data-src="trash-classification/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png"></p>
<p>其中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出，输入层的神经元只是输入。</p>
<ul>
<li>神经网络的特点<ul>
<li>每个连接都有个权值</li>
<li>同一层神经元之间没有连接</li>
<li>最后的输出结果对应的层也称之为<strong>全连接层</strong></li>
</ul>
</li>
</ul>
<p>神经网络是深度学习的重要算法，在图像（如图像的分类、检测）和自然语言处理（如文本分类、聊天等）有很多应用。</p>
<h3 id="神经网络原理"><a href="#神经网络原理" class="headerlink" title="神经网络原理"></a>神经网络原理</h3><p>神经网络分类的原理是怎么样的？我们还是围绕着损失、优化这两块去说。神经网络输出结果如何分类？</p>
<p><img data-src="trash-classification/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E5%88%86%E7%B1%BB.png"></p>
<p><strong>神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数。</strong></p>
<p>任意事件发生的概率都在0和1之间，且总有某一个事件发生（概率的和为1）。如果将分类问题中“一个样例属于某一个类别”看成一个概率事件，那么训练数据的正确答案就符合一个概率分布。如何将神经网络前向传播得到的结果也变成概率分布呢？Softmax回归就是一个非常常用的方法。</p>
<h4 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h4><p>Softmax回归将神经网络输出转换成概率结果</p>
<p>$$softmax(y_i) = \frac{e^{y_i}}{\sum_{j=1}^{n}e^{y_i}}$$<br><img data-src="trash-classification/softmax%E5%9B%9E%E5%BD%92.png"></p>
<p><img data-src="trash-classification/softmax%E5%B1%95%E5%BC%80.png"></p>
<p>如何理解这个公式的作用呢？看一下计算案例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">假设输出结果为：<span class="number">2.3</span>, <span class="number">4.1</span>, <span class="number">5.6</span></span><br><span class="line">softmax的计算输出结果为：</span><br><span class="line">y1_p = e^<span class="number">2.3</span>/(e^<span class="number">2.3</span>+e^<span class="number">4.1</span>+e^<span class="number">5.6</span>)</span><br><span class="line">y2_p = e^<span class="number">4.1</span>/(e^<span class="number">2.3</span>+e^<span class="number">4.1</span>+e^<span class="number">5.6</span>)</span><br><span class="line">y3_p = e^<span class="number">5.6</span>/(e^<span class="number">2.3</span>+e^<span class="number">4.1</span>+e^<span class="number">5.6</span>)</span><br></pre></td></tr></table></figure>

<p><strong>这样就把神经网络的输出变成了概率输出</strong><br>那么如何去衡量神经网络预测的概率分布和真实答案的概率分布之间的距离？</p>
<h4 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h4><p>公式：</p>
<p>$$H_{y^{‘}}(y)= - \sum_{i}y_{i}^{‘} log(y_i)$$</p>
<p>为了能够衡量距离，目标值需要进行one-hot编码，能与概率值一一对应，如下图</p>
<p><img data-src="trash-classification/%E4%BA%A4%E5%8F%89%E6%8D%9F%E5%A4%B1%E7%90%86%E8%A7%A3.png"></p>
<p>损失值如何计算？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-(<span class="number">0l</span>og(<span class="number">0.10</span>)+<span class="number">0l</span>og(<span class="number">0.05</span>)+<span class="number">0l</span>og(<span class="number">0.15</span>)+<span class="number">0l</span>og(<span class="number">0.10</span>)+<span class="number">0l</span>og(<span class="number">0.05</span>)+<span class="number">0l</span>og(<span class="number">0.20</span>)</span><br><span class="line">    +<span class="number">1l</span>og(<span class="number">0.10</span>)+<span class="number">0l</span>og(<span class="number">0.05</span>)+<span class="number">0l</span>og(<span class="number">0.10</span>)+<span class="number">0l</span>og(<span class="number">0.10</span>))</span><br></pre></td></tr></table></figure>
<p>上述的结果为-1log(0.10)，那么为了减少这一个样本的损失。神经网络应该怎么做？所以会提高对应目标值为1的位置输出概率大小，下面是log函数，log函数取正号之后单调递增的，取负号之后就单调递减。所以在减小损失值时相应的就会增大对应的概率值，同时根据softmax公式，其他类别的概率必定会减少。所以这里会提高对应目标值为1的位置输出概率大小。</p>
<img data-src="trash-classification/image-20190818150116295.png" align='center' width='500'>



<p>那么神经网络是怎么来减少损失值或者是找到最小的损失值呢？</p>
<h4 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h4><p>目的：使损失函数的值找到最小值</p>
<p>函数的<strong>梯度（gradient）</strong>指出了函数的最陡增长方向。<strong>沿着梯度的方向走，函数增长得就越快。那么按梯度的负方向走，函数值自然就降低得最快了</strong>。模型的训练目标即是寻找合适的$w$与$b$以最小化损失函数值。假设**$w$与$b$都是一维实数**，那么可以得到如下的$J$关于$w$与$b$的图：</p>
<p><img data-src="trash-classification/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%9B%BE.png"></p>
<p>可以看到，此损失函数$J$是一个<strong>凸函数</strong></p>
<p>参数w和b的更新公式为：<br>$$w := w - \alpha\frac{dJ(w, b)}{dw}$$</p>
<p>$$b := b - \alpha\frac{dJ(w, b)}{db}$$</p>
<blockquote>
<p>注：其中 α 表示学习速率，即每次更新的 w 的步伐长度。当 w 大于最优解 w′ 时，导数大于 0，那么 w 就会向更小的方向更新。反之当 w 小于最优解 w′ 时，导数小于 0，那么 w 就会向更大的方向更新。迭代直到收敛。</p>
</blockquote>
<p>通过平面来理解梯度下降过程：</p>
<p><img data-src="trash-classification/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%90%86%E8%A7%A3.png"></p>
<h4 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h4><p>反向传播算法实际就是：<strong>我们使用链式求导法则，反向层层推进，计算出每一层神经节点的偏导数，然后使用梯度下降，不断调整每一个节点的权重，从而达到求得全局最小值的目的。</strong></p>
<h2 id="案例：Mnist手写数字识别"><a href="#案例：Mnist手写数字识别" class="headerlink" title="案例：Mnist手写数字识别"></a>案例：Mnist手写数字识别</h2><h3 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h3><p><img data-src="trash-classification/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97.png"></p>
<p>文件说明：</p>
<ul>
<li>train-images-idx3-ubyte.gz: training set images (9912422 bytes)</li>
<li>train-labels-idx1-ubyte.gz: training set labels (28881 bytes)</li>
<li>t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)</li>
<li>t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)</li>
</ul>
<blockquote>
<p>网址：<span class="exturl" data-url="aHR0cDovL3lhbm4ubGVjdW4uY29tL2V4ZGIvbW5pc3Qv">http://yann.lecun.com/exdb/mnist/<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p>Mnist数据集可以从官网下载，网址： <strong><span class="exturl" data-url="aHR0cDovL3lhbm4ubGVjdW4uY29tL2V4ZGIvbW5pc3Qv">http://yann.lecun.com/exdb/mnist/<i class="fa fa-external-link-alt"></i></span></strong> 下载下来的数据集被分成两部分：55000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。每一个MNIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签。我们把这些图片设为“xs”，把这些标签设为“ys”。训练数据集和测试数据集都包含xs和ys，比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。</p>
<p><img data-src="trash-classification/%E6%95%B0%E5%AD%97.png"><br>我们可以知道图片是黑白图片，每一张图片包含28像素X28像素。我们把这个数组展开成一个向量，长度是 28x28 = 784。因此，在MNIST训练数据集中，mnist.train.images 是一个形状为 [60000, 784] 的张量。</p>
<p><img data-src="trash-classification/%E6%95%B0%E5%AD%97%E5%BC%A0%E9%87%8F.png"></p>
<p>MNIST中的每个图像都具有相应的标签，0到9之间的数字表示图像中绘制的数字。用的是one-hot编码</p>
<p><img data-src="trash-classification/%E6%95%B0%E5%AD%97label.png"></p>
<h3 id="Mnist数据获取API"><a href="#Mnist数据获取API" class="headerlink" title="Mnist数据获取API"></a>Mnist数据获取API</h3><p>TensorFlow框架自带了读取这个数据集的接口：</p>
<ul>
<li>from tensorflow.examples.tutorials.mnist import input_data<ul>
<li>mnist = input_data.read_data_sets(path, one_hot=True)<ul>
<li>mnist.train.next_batch(100)(提供批量获取功能)</li>
<li>mnist.train.images、labels</li>
<li>mnist.test.images、labels</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="流程分析"><a href="#流程分析" class="headerlink" title="流程分析"></a>流程分析</h3><h4 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h4><p>我们采用只有一层，即最后一个输出层的神经网络，也称之为全连接层神经网络。</p>
<p><img data-src="trash-classification/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1.png"></p>
<h4 id="相关计算"><a href="#相关计算" class="headerlink" title="相关计算"></a>相关计算</h4><ul>
<li>tf.matmul(a, b, name=None)+bias<ul>
<li>return:全连接结果，供交叉损失运算</li>
</ul>
</li>
<li>tf.train.GradientDescentOptimizer(learning_rate)<ul>
<li>梯度下降</li>
<li>learning_rate:学习率</li>
<li>method:<ul>
<li>minimize(loss):最小优化损失</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h4><ul>
<li>获取数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;.&#x2F;mnist_data&#x2F;&quot;, one_hot&#x3D;True)</span><br></pre></td></tr></table></figure>

<ul>
<li>定义数据占位符，Mnist数据实时提供给placeholder</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、准备数据</span></span><br><span class="line"><span class="comment"># x [None, 784] y_true [None. 10]</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;mnist_data&quot;</span>):</span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_true = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>全连接结果计算</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2、全连接层神经网络计算</span></span><br><span class="line"><span class="comment"># 类别：10个类别  全连接层：10个神经元</span></span><br><span class="line"><span class="comment"># 参数w: [784, 10]   b:[10]</span></span><br><span class="line"><span class="comment"># 全连接层神经网络的计算公式：[None, 784] * [784, 10] + [10] = [None, 10]</span></span><br><span class="line"><span class="comment"># 随机初始化权重偏置参数，这些是优化的参数，必须使用变量op去定义</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;fc_model&quot;</span>):</span><br><span class="line">    weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">10</span>], mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>), name=<span class="string">&quot;w&quot;</span>)</span><br><span class="line">    bias = tf.Variable(tf.random_normal([<span class="number">10</span>], mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>), name=<span class="string">&quot;b&quot;</span>)</span><br><span class="line">    <span class="comment"># fc层的计算</span></span><br><span class="line">    <span class="comment"># y_predict [None, 10]输出结果，提供给softmax使用</span></span><br><span class="line">    y_predict = tf.matmul(x, weights) + bias</span><br></pre></td></tr></table></figure>

<ul>
<li>损失计算与优化</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3、softmax回归以及交叉熵损失计算</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;loss&quot;</span>):</span><br><span class="line">    <span class="comment"># labels:真实值 [None, 10]  one_hot</span></span><br><span class="line">    <span class="comment"># logits:全脸层的输出[None,10]</span></span><br><span class="line">    <span class="comment"># 返回每个样本的损失组成的列表</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,</span><br><span class="line">                                                                  logits=y_predict))</span><br><span class="line"><span class="comment"># 4、梯度下降损失优化</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;optimizer&quot;</span>):</span><br><span class="line">    <span class="comment"># 学习率</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br></pre></td></tr></table></figure>

<ul>
<li>模型训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启会话去训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 初始化变量</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">if</span> FLAGS.is_train == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 循环步数去训练</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            <span class="comment"># 获取数据，实时提供</span></span><br><span class="line">            <span class="comment"># 每步提供50个样本训练          </span></span><br><span class="line">                mnist_x, mnist_y = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">                _, loss_value = sess.run([optimizer, loss], feed_dict=&#123;x:mnist_x, y:mnist_y&#125;)</span><br><span class="line">                print(<span class="string">&#x27;第%d次训练, 损失值%.4f&#x27;</span> %(i+<span class="number">1</span>, loss_value))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="完善模型功能"><a href="#完善模型功能" class="headerlink" title="完善模型功能"></a>完善模型功能</h4><ul>
<li><p>如何计算准确率</p>
<ul>
<li><p>equal_list = tf.equal(tf.argmax(y, 1), tf.argmax(y_label, 1))</p>
</li>
<li><p>accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))</p>
<p><img data-src="trash-classification/%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97.png"></p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5、得出每次训练的准确率（通过真实值和预测值进行位置比较，每个样本都比较）</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;accuracy&quot;</span>):</span><br><span class="line">    equal_list = tf.equal(tf.argmax(y_true, <span class="number">1</span>), tf.argmax(y_predict, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))</span><br></pre></td></tr></table></figure>
<ul>
<li>使用测试集评估模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6、定义一个flag判断是否是训练模式</span></span><br><span class="line">tf.app.flags.DEFINE_integer(<span class="string">&quot;is_train&quot;</span>, <span class="number">1</span>, <span class="string">&quot;指定是否是训练模型，还是拿数据去预测&quot;</span>)</span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line">...</span><br><span class="line"><span class="comment"># 每次拿十个样本预测</span></span><br><span class="line">mnist_x, mnist_y = mnist.test.next_batch(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="卷积神经网络简介"><a href="#卷积神经网络简介" class="headerlink" title="卷积神经网络简介"></a>卷积神经网络简介</h3><ul>
<li>传统意义上的多层神经网络是只有输入层、隐藏层、输出层。其中隐藏层的层数根据需要而定，没有明确的理论推导来说明到底多少层合适</li>
<li>卷积神经网络CNN，在原来多层神经网络的基础上，加入了更加有效的特征学习部分，具体操作就是在原来的全连接层前面加入了卷积层与池化层。<strong>卷积神经网络出现，使得神经网络层数得以加深，“深度”学习由此而来。</strong></li>
</ul>
<blockquote>
<p>通常所说的深度学习，<strong>一般指的是这些CNN等新的结构以及一些新的方法（比如新的激活函数Relu等）</strong>，解决了传统多层神经网络的一些难以解决的问题</p>
</blockquote>
<h3 id="卷积神经网络原理"><a href="#卷积神经网络原理" class="headerlink" title="卷积神经网络原理"></a>卷积神经网络原理</h3><p>先来看一个示意图：</p>
<p><img data-src="trash-classification/%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%84%E6%A6%82%E8%BF%B0.png"></p>
<h4 id="卷积神经网络三个结构"><a href="#卷积神经网络三个结构" class="headerlink" title="卷积神经网络三个结构"></a>卷积神经网络三个结构</h4><p>神经网络(neural networks)的基本组成包括输入层、隐藏层、输出层。而卷积神经网络的特点在于隐藏层分为卷积层和池化层(pooling layer，又叫下采样层)以及激活层。每一层的作用</p>
<ul>
<li>卷积层：通过在原始图像上平移来提取特征</li>
<li>激活层：增加非线性分割能力</li>
<li>池化层：减少学习的参数，降低网络的复杂度（最大池化和平均池化）</li>
</ul>
<p>为了能够达到分类效果，还会有一个全连接层(Full Connection)也就是最后的输出层，进行损失计算并输出分类结果。</p>
<h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p><img data-src="trash-classification/%E5%8D%B7%E7%A7%AF%E5%B1%82.png" alt="卷积层"></p>
<p><strong>参数及结构</strong></p>
<p>四个超参数控制输出体积的大小：过滤器大小，深度，步幅和零填充。得到的每一个深度也叫一个Feature Map。</p>
<p><strong>卷积层的处理</strong>，在卷积层有一个重要的就是过滤器大小（需要自己指定），若输入值是一个[32x32x3]的大小（例如RGB CIFAR-10彩色图像）。如果每个过滤器（Filter）的大小为5×5，则CNN层中的每个Filter将具有对输入体积中的[5x5x3]区域的权重，总共5 <em>5</em> 3 = 75个权重（和+1偏置参数），输入图像的3个深度分别与Filter的3个深度进行运算。请注意，沿着深度轴的连接程度必须为3，因为这是输入值的深度，并且也要记住这只是一个Filter。</p>
<ul>
<li>假设输入卷的大小为[16x16x20]。然后使用3x3的示例接收字段大小，CNN中的每个神经元现在将具有总共3 <em>3</em> 20 = 180个连接到输入层的连接。</li>
</ul>
<p><strong>卷积层的输出深度</strong>，那么一个卷积层的输出深度是可以指定的，输出深度是由你本次卷积中Filter的个数决定。加入上面我们使用了64个Filter，也就是[5,5,3,64]，这样就得到了64个Feature Map，这样这64个Feature Map可以作为下一次操作的输入值</p>
<p><strong>卷积层的输出宽度</strong>，输出宽度可以通过特定算数公式进行得出，后面会列出公式。</p>
<h5 id="卷积输出值的计算"><a href="#卷积输出值的计算" class="headerlink" title="卷积输出值的计算"></a>卷积输出值的计算</h5><p>我们用一个简单的例子来讲述如何计算卷积，然后，我们抽象出卷积层的一些重要概念和计算方法。</p>
<p>假设有一个5<em>5的图像，使用一个3</em>3的filter进行卷积，得到了到一个3<em>3的Feature Map，至于得到3</em>3大小，可以自己去计算一下。如下所示：</p>
<p><img data-src="trash-classification/%E8%AE%A1%E7%AE%97%E5%9B%BE1.png" alt="计算图1"></p>
<p>我们看下它的计算过程，首先计算公式如下：</p>
<p><img data-src="trash-classification/%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png" alt="计算公式"></p>
<p>根据计算的例子，第一次：</p>
<p><img data-src="trash-classification/%E8%AE%A1%E7%AE%97%E5%9B%BE2.png" alt="计算图2"></p>
<p>第二次：</p>
<p><img data-src="trash-classification/%E8%AE%A1%E7%AE%97%E5%9B%BE3.png" alt="计算图3"></p>
<p>通过这样我们可以依次计算出Feature Map中所有元素的值。下面的动画显示了整个Feature Map的计算过程：</p>
<p><img data-src="trash-classification/%E8%AE%A1%E7%AE%97%E5%8A%A8%E5%9B%BE.gif" alt="计算动图"></p>
<p><strong>步长</strong></p>
<p>那么在卷积神经网络中有一个概念叫步长，也就是Filter移动的间隔大小。上面的计算过程中，步幅(stride)为1。步幅可以设为大于1的数。例如，当步幅为2时，我们可以看到得出2*2大小的Feature Map，发现这也跟步长有关。Feature Map计算如下：</p>
<p><img data-src="trash-classification/%E6%AD%A5%E9%95%BF%E4%B8%BA2.1.png" alt="步长为2.1"></p>
<p><img data-src="trash-classification/%E6%AD%A5%E9%95%BF%E4%B8%BA2.2.png" alt="步长为2.2"></p>
<p><img data-src="trash-classification/%E6%AD%A5%E9%95%BF%E4%B8%BA2.3.png" alt="步长为2.3"></p>
<p><img data-src="trash-classification/%E6%AD%A5%E9%95%BF%E4%B8%BA2.4.png" alt="步长为2.4"></p>
<h5 id="填充和多Filter"><a href="#填充和多Filter" class="headerlink" title="填充和多Filter"></a>填充和多Filter</h5><p>我们前面还曾提到，每个卷积层可以有多个filter。每个filter和原始图像进行卷积后，都可以得到一个Feature Map。因此，卷积后Feature Map的深度(个数)和卷积层的filter个数是相同的。</p>
<p>如果我们的步长移动与filter的大小不适合，导致不能正好移动到边缘怎么办？</p>
<p><img data-src="trash-classification/%E5%A4%9A%E4%B8%AAFilter.gif" alt="多个Filter"></p>
<p>以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且filter的权值对于上一层所有神经元都是一样的。</p>
<p><strong>总结输出大小</strong></p>
<ul>
<li>输入体积大小$H_1<em>W_1</em>D_1$</li>
<li>四个超参数：<ul>
<li>Filter数量<em>K</em></li>
<li>Filter大小<em>F</em></li>
<li>步长<em>S</em></li>
<li>零填充大小<em>P</em></li>
</ul>
</li>
<li>输出体积大小$H_2<em>W_2</em>D_2$<ul>
<li>$H_2 = (H_1 - F + 2P)/S + 1$</li>
<li>$W_2 = (W_1 - F + 2P)/S + 1$</li>
<li>$D_2 = K$</li>
</ul>
</li>
</ul>
<h4 id="激活函数-Relu"><a href="#激活函数-Relu" class="headerlink" title="激活函数-Relu"></a>激活函数-Relu</h4><p>一般在进行卷积之后就会提供给激活函数得到一个输出值。我们不使用$sigmoid$，$softmax$，而使用$Relu$。该激活函数的定义是：</p>
<p>$f(u)= max(0,u)$</p>
<p>Relu函数如下：</p>
<p><img data-src="trash-classification/Relu.png" alt="Relu"></p>
<p><strong>特点</strong></p>
<ul>
<li>速度快：与sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,u)，计算代价小很多</li>
<li>稀疏性： 因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。</li>
</ul>
<h4 id="池化计算"><a href="#池化计算" class="headerlink" title="池化计算"></a>池化计算</h4><p>池化层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。池化的方法很多，最常用的是Max Pooling。Max Pooling实际上就是在$n$个样本中取最大值，作为采样后的样本值。下图是max pooling：</p>
<p><img data-src="trash-classification/Pooling.png" alt="Pooling"></p>
<p>除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。对于深度为D的Feature Map，各层独立做Pooling，因此Pooling后的深度仍然为D。</p>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权，最后的全连接层在整个卷积神经网络中起到“分类器”的作用。</p>
<h3 id="实例探究"><a href="#实例探究" class="headerlink" title="实例探究"></a>实例探究</h3><p>卷积网络领域有几种架构，名称。最常见的是：</p>
<ul>
<li>LeNet：卷积网络的第一个成功应用是由Yann LeCun于1990年代开发的。其中最著名的是LeNet架构，用于读取邮政编码，数字等。</li>
<li>AlexNet：卷积网络在计算机视觉中的第一个应用是AlexNet，由亚历克斯·克里维斯基，伊利亚·萨茨基弗和吉奥夫·欣顿发展。AlexNet在2012年被提交给ImageNet ILSVRC挑战，明显优于第二名。该网络与LeNet具有非常相似的体系结构，但是使用更多层数，更大和更具特色的卷积层。</li>
<li>ZFNet。ILSVRC 2013获奖者是Matthew Zeiler和Rob Fergus的卷积网络。它被称为ZFNet（Zeiler＆Fergus Net的缩写）。通过调整架构超参数，特别是通过扩展中间卷积层的大小，使第一层的步幅和过滤器尺寸更小，对AlexNet的改进。</li>
<li>GoogleNet。ILSVRC 2014获奖者是Szegedy等人的卷积网络。来自Google。其主要贡献是开发一个初始模块，大大减少了网络中的参数数量（4M，与AlexNet的60M相比）。GoogLeNet还有几个后续版本，最近的是Inception-v4。</li>
<li>VGGNet。2011年ILSVRC的亚军是来自Karen Simonyan和Andrew Zisserman的网络，被称为VGGNet。它的主要贡献在于表明网络的深度是良好性能的关键组成部分。他们最终的网络包含16个CONV / FC层，并且具有非常均匀的架构，从始至终只能执行3x3卷积和2x2池化。VGGNet的缺点是使用更多的内存和参数更多。</li>
<li>ResNet。Kaiming He等人开发的残差网络 是ILSVRC 2015的获胜者。它大量使用特殊的跳过连接和批量归一化。该架构在网络末端也没有使用全连接层。ResNets目前是迄今为止最先进的卷积神经网络模型。</li>
</ul>
<p>下面就是VGGNet的结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">INPUT: [224x224x3]        memory:  224*224*3&#x3D;150K   weights: 0</span><br><span class="line">CONV3-64: [224x224x64]  memory:  224*224*64&#x3D;3.2M   weights: (3*3*3)*64 &#x3D; 1,728</span><br><span class="line">CONV3-64: [224x224x64]  memory:  224*224*64&#x3D;3.2M   weights: (3*3*64)*64 &#x3D; 36,864</span><br><span class="line">POOL2: [112x112x64]  memory:  112*112*64&#x3D;800K   weights: 0</span><br><span class="line">CONV3-128: [112x112x128]  memory:  112*112*128&#x3D;1.6M   weights: (3*3*64)*128 &#x3D; 73,728</span><br><span class="line">CONV3-128: [112x112x128]  memory:  112*112*128&#x3D;1.6M   weights: (3*3*128)*128 &#x3D; 147,456</span><br><span class="line">POOL2: [56x56x128]  memory:  56*56*128&#x3D;400K   weights: 0</span><br><span class="line">CONV3-256: [56x56x256]  memory:  56*56*256&#x3D;800K   weights: (3*3*128)*256 &#x3D; 294,912</span><br><span class="line">CONV3-256: [56x56x256]  memory:  56*56*256&#x3D;800K   weights: (3*3*256)*256 &#x3D; 589,824</span><br><span class="line">CONV3-256: [56x56x256]  memory:  56*56*256&#x3D;800K   weights: (3*3*256)*256 &#x3D; 589,824</span><br><span class="line">POOL2: [28x28x256]  memory:  28*28*256&#x3D;200K   weights: 0</span><br><span class="line">CONV3-512: [28x28x512]  memory:  28*28*512&#x3D;400K   weights: (3*3*256)*512 &#x3D; 1,179,648</span><br><span class="line">CONV3-512: [28x28x512]  memory:  28*28*512&#x3D;400K   weights: (3*3*512)*512 &#x3D; 2,359,296</span><br><span class="line">CONV3-512: [28x28x512]  memory:  28*28*512&#x3D;400K   weights: (3*3*512)*512 &#x3D; 2,359,296</span><br><span class="line">POOL2: [14x14x512]  memory:  14*14*512&#x3D;100K   weights: 0</span><br><span class="line">CONV3-512: [14x14x512]  memory:  14*14*512&#x3D;100K   weights: (3*3*512)*512 &#x3D; 2,359,296</span><br><span class="line">CONV3-512: [14x14x512]  memory:  14*14*512&#x3D;100K   weights: (3*3*512)*512 &#x3D; 2,359,296</span><br><span class="line">CONV3-512: [14x14x512]  memory:  14*14*512&#x3D;100K   weights: (3*3*512)*512 &#x3D; 2,359,296</span><br><span class="line">POOL2: [7x7x512]  memory:  7*7*512&#x3D;25K  weights: 0</span><br><span class="line">FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 &#x3D; 102,760,448</span><br><span class="line">FC: [1x1x4096]  memory:  4096  weights: 4096*4096 &#x3D; 16,777,216</span><br><span class="line">FC: [1x1x1000]  memory:  1000 weights: 4096*1000 &#x3D; 4,096,000</span><br><span class="line"></span><br><span class="line">TOTAL memory: 24M * 4 bytes ~&#x3D; 93MB &#x2F; image (only forward! ~*2 for bwd)</span><br><span class="line">TOTAL params: 138M parameters</span><br></pre></td></tr></table></figure>



<h2 id="Inception-ResNet-v2介绍"><a href="#Inception-ResNet-v2介绍" class="headerlink" title="Inception-ResNet-v2介绍"></a>Inception-ResNet-v2介绍</h2><h3 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h3><p><strong>问题：</strong></p>
<ul>
<li>图像中突出部分的大小差别很大。例如，狗的图像可以是以下任意情况。每张图像中狗所占区域都是不同的。</li>
</ul>
<p><img data-src="trash-classification/1527656989598.png" alt="img"></p>
<p><em>从左到右：狗占据图像的区域依次减小</em></p>
<ul>
<li>由于信息位置的巨大差异，为卷积操作选择合适的卷积核大小就比较困难。信息分布更全局性的图像偏好较大的卷积核，信息分布比较局部的图像偏好较小的卷积核。</li>
<li>非常深的网络更容易过拟合。将梯度更新传输到整个网络是很困难的。</li>
<li>简单地堆叠较大的卷积层非常消耗计算资源。</li>
</ul>
<p><strong>解决方案：</strong></p>
<p>为什么不在同一层级上运行具备多个尺寸的滤波器呢？网络本质上会变得稍微「宽一些」，而不是「更深」。作者因此设计了 Inception 模块。</p>
<p>下图是「原始」Inception 模块。它使用 3 个不同大小的滤波器（1x1、3x3、5x5）对输入执行卷积操作，此外它还会执行最大池化。所有子层的输出最后会被级联起来，并传送至下一个 Inception 模块。</p>
<p><img data-src="trash-classification/1527656989203.png" alt="img"></p>
<p><em>原始 Inception 模块。</em></p>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>随着网络的加深，出现了训练集准确率下降的现象，因为梯度反向传播到前面的层，重复相乘可能使梯度无穷小。结果就是，随着网络的层数更深，其性能趋于饱和，甚至开始迅速下降。所以作者针对这个问题提出了一种全新的网络，叫深度残差网络（ResNet）其核心思想是引入一个所谓的「恒等快捷连接」（identity shortcut connection），直接跳过一个或多个层，如下图所示：</p>
<p> <img data-src="trash-classification/ResNet.png" alt="img"></p>
<h3 id="Inception-ResNet-v2"><a href="#Inception-ResNet-v2" class="headerlink" title="Inception-ResNet-v2"></a>Inception-ResNet-v2</h3><p>Google团队发布**Inception-ResNet-v2，它在ILSVRC图像分类基准测试中实现了当下最好的成绩。Inception-ResNet-v2是早期Inception V3模型变化而来，从微软的残差网络（ResNet）论文中得到了一些灵感。</p>
<p><img data-src="trash-classification/90.jpeg"></p>
<h2 id="使用Inception-ResNet-v2进行垃圾分类"><a href="#使用Inception-ResNet-v2进行垃圾分类" class="headerlink" title="使用Inception-ResNet-v2进行垃圾分类"></a>使用Inception-ResNet-v2进行垃圾分类</h2><h3 id="数据集介绍-1"><a href="#数据集介绍-1" class="headerlink" title="数据集介绍"></a>数据集介绍</h3><p>下载链接<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dhcnl0aHVuZy90cmFzaG5ldC9ibG9iL21hc3Rlci9kYXRhL2RhdGFzZXQtcmVzaXplZC56aXA=">https://github.com/garythung/trashnet/blob/master/data/dataset-resized.zip<i class="fa fa-external-link-alt"></i></span></p>
<p>数据集一共分了六个类别，分别是：cardboard glass metal paper plastic trash，一共2527个样本</p>
<p><img data-src="trash-classification/glass108.jpg" alt="glass108"></p>
<p><em>glass样本示例</em></p>
<h3 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h3><h4 id="数据预处理："><a href="#数据预处理：" class="headerlink" title="数据预处理："></a>数据预处理：</h4><ul>
<li><p>设置数据类别</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class_names_to_ids = &#123;<span class="string">&#x27;cardboard&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;glass&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;metal&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;paper&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;plastic&#x27;</span>:<span class="number">4</span>, <span class="string">&#x27;trash&#x27;</span>:<span class="number">5</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>把文件名及类别写入文件</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">data_dir = <span class="string">&#x27;dataset/&#x27;</span></span><br><span class="line">output_path = <span class="string">&#x27;list.txt&#x27;</span></span><br><span class="line">fd = open(output_path, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> class_name <span class="keyword">in</span> class_names_to_ids.keys():</span><br><span class="line">    images_list = os.listdir(data_dir + class_name)</span><br><span class="line">    <span class="keyword">for</span> image_name <span class="keyword">in</span> images_list:</span><br><span class="line">        fd.write(<span class="string">&#x27;&#123;&#125;/&#123;&#125; &#123;&#125;\n&#x27;</span>.format(class_name, image_name, class_names_to_ids[class_name]))</span><br><span class="line">fd.close()</span><br></pre></td></tr></table></figure>

<ul>
<li>划分训练集合测试集</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机选取样本做训练集和测试集</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">_NUM_VALIDATION = <span class="number">505</span></span><br><span class="line">_RANDOM_SEED = <span class="number">0</span></span><br><span class="line">list_path = <span class="string">&#x27;list.txt&#x27;</span></span><br><span class="line">train_list_path = <span class="string">&#x27;list_train.txt&#x27;</span></span><br><span class="line">val_list_path = <span class="string">&#x27;list_val.txt&#x27;</span></span><br><span class="line">fd = open(list_path)</span><br><span class="line">lines = fd.readlines()</span><br><span class="line">fd.close()</span><br><span class="line">random.seed(_RANDOM_SEED)</span><br><span class="line">random.shuffle(lines)</span><br><span class="line">fd = open(train_list_path, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines[_NUM_VALIDATION:]:</span><br><span class="line">    fd.write(line)</span><br><span class="line">fd.close()</span><br><span class="line">fd = open(val_list_path, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines[:_NUM_VALIDATION]:</span><br><span class="line">    fd.write(line)</span><br><span class="line">fd.close()</span><br></pre></td></tr></table></figure>

<ul>
<li><p>加载数据</p>
<ul>
<li>解析训练集测试集文件名</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_test_data</span>(<span class="params">list_file</span>):</span></span><br><span class="line">    list_train = open(list_file)</span><br><span class="line">    x_train = []</span><br><span class="line">    y_train = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> list_train.readlines():</span><br><span class="line">        x_train.append(line.strip()[:<span class="number">-2</span>])</span><br><span class="line">        y_train.append(int(line.strip()[<span class="number">-1</span>]))</span><br><span class="line">        <span class="comment">#print(line.strip())</span></span><br><span class="line">    <span class="keyword">return</span> x_train, y_train</span><br><span class="line">x_train, y_train = get_train_test_data(<span class="string">&#x27;list_train.txt&#x27;</span>)</span><br><span class="line">x_test, y_test = get_train_test_data(<span class="string">&#x27;list_val.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>加载并预处理数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_train_test_data</span>(<span class="params">x_path</span>):</span></span><br><span class="line">    images = []</span><br><span class="line">    <span class="keyword">for</span> image_path <span class="keyword">in</span> x_path:</span><br><span class="line">        img_load = load_img(<span class="string">&#x27;dataset/&#x27;</span>+image_path)</span><br><span class="line">        img = image.img_to_array(img_load)</span><br><span class="line">        img = preprocess_input(img)</span><br><span class="line">        images.append(img)</span><br><span class="line">    <span class="keyword">return</span> images</span><br><span class="line">train_images = process_train_test_data(x_train)</span><br><span class="line">test_images = process_train_test_data(x_test)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="构造模型"><a href="#构造模型" class="headerlink" title="构造模型"></a>构造模型</h4><p>这里使用预先keras中已经训练好的InceptionResNetV2模型，模型的最后一层全连接层加载模型时不加载，按照我们需要分类的类别数进行设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications.inception_resnet_v2 <span class="keyword">import</span> InceptionResNetV2</span><br><span class="line">base_model = InceptionResNetV2(include_top=<span class="literal">False</span>, pooling=<span class="string">&#x27;avg&#x27;</span>)</span><br><span class="line">outputs = Dense(<span class="number">6</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)(base_model.output)</span><br><span class="line">model = Model(base_model.inputs, outputs)</span><br></pre></td></tr></table></figure>

<h4 id="模型训练与保存"><a href="#模型训练与保存" class="headerlink" title="模型训练与保存"></a>模型训练与保存</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置ModelCheckpoint，按照验证集的准确率进行保存</span></span><br><span class="line">save_dir=<span class="string">&#x27;train_model&#x27;</span></span><br><span class="line">filepath=<span class="string">&quot;model_&#123;epoch:02d&#125;-&#123;val_acc:.2f&#125;.hdf5&quot;</span></span><br><span class="line">checkpoint = ModelCheckpoint(os.path.join(save_dir, filepath), monitor=<span class="string">&#x27;val_acc&#x27;</span>,verbose=<span class="number">1</span>, </span><br><span class="line">                            save_best_only=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 模型设置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acc_top3</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> top_k_categorical_accuracy(y_true, y_pred, k=<span class="number">3</span>)</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acc_top5</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> top_k_categorical_accuracy(y_true, y_pred, k=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>, acc_top3, acc_top5])</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">model.fit(np.array(train_images), to_categorical(y_train),</span><br><span class="line">          batch_size=<span class="number">8</span>,</span><br><span class="line">          epochs=<span class="number">5</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          validation_data=(np.array(test_images), to_categorical(y_test)),</span><br><span class="line">          callbacks=[checkpoint])</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 训练结果输出示例</span></span><br><span class="line">Train on 2022 samples, validate on 505 samples</span><br><span class="line">Epoch 1/5</span><br><span class="line">2022/2022 [==============================] - 238s 118ms/step - loss: 0.1213 - acc: 0.9629 - acc_top3: 0.9970 - acc_top5: 0.9990 - val_loss: 0.6070 - val_acc: 0.8653 - val_acc_top3: 0.9644 - val_acc_top5: 0.9921</span><br><span class="line">Epoch 00001: val_acc improved from 0.86139 to 0.86535, saving model to train_model/model_01-0.87.hdf5</span><br></pre></td></tr></table></figure>

<h4 id="模型加载与预测"><a href="#模型加载与预测" class="headerlink" title="模型加载与预测"></a>模型加载与预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载指定模型</span></span><br><span class="line">model.load_weights(<span class="string">&#x27;train_model/model_01-0.87.hdf5&#x27;</span>)</span><br><span class="line"><span class="comment"># 直接使用predict方法进行预测</span></span><br><span class="line">y_pred = model.predict(np.array(test_images))</span><br></pre></td></tr></table></figure>

<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>可以看到在验证集上的top1准确率是86.53%，top3的准确率是96.44%，相比于Inception-ResNet-v2模型本身的准确率有一些提升，也说明我们在Inception-ResNet-v2微调之后的这个垃圾分类模型充分利用了原模型已经学习到的规律，并对我们这个特定数据集有很好的预测能力。</p>
<h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><p>相关链接：</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL2F2NjU2OTc0NjA=">https://www.bilibili.com/video/av65697460<i class="fa fa-external-link-alt"></i></span></li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>飞木鱼
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://liufengyu.cn/posts/trash-classification.html" title="【深度学习】手把手教你用卷积神经网络做垃圾分类">https://liufengyu.cn/posts/trash-classification.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/tensorflow/" rel="tag"><i class="fa fa-tag"></i> Tensorflow</a>
              <a href="/tags/keras/" rel="tag"><i class="fa fa-tag"></i> Keras</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/wsl-terminal-color.html" rel="prev" title="WIN10 WSL VIM颜色调整">
      <i class="fa fa-chevron-left"></i> WIN10 WSL VIM颜色调整
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/scala-learning.html" rel="next" title="Scala学习指南">
      Scala学习指南 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B"><span class="nav-number">2.1.</span> <span class="nav-text">神经网络简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86"><span class="nav-number">2.2.</span> <span class="nav-text">神经网络原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax%E5%9B%9E%E5%BD%92"><span class="nav-number">2.2.1.</span> <span class="nav-text">softmax回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="nav-number">2.2.2.</span> <span class="nav-text">交叉熵损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="nav-number">2.2.3.</span> <span class="nav-text">梯度下降算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="nav-number">2.2.4.</span> <span class="nav-text">反向传播算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B%EF%BC%9AMnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">3.</span> <span class="nav-text">案例：Mnist手写数字识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="nav-number">3.1.</span> <span class="nav-text">数据集介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mnist%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96API"><span class="nav-number">3.2.</span> <span class="nav-text">Mnist数据获取API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90"><span class="nav-number">3.3.</span> <span class="nav-text">流程分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.3.1.</span> <span class="nav-text">网络设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E8%AE%A1%E7%AE%97"><span class="nav-number">3.3.2.</span> <span class="nav-text">相关计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="nav-number">3.3.3.</span> <span class="nav-text">具体流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%8C%E5%96%84%E6%A8%A1%E5%9E%8B%E5%8A%9F%E8%83%BD"><span class="nav-number">3.3.4.</span> <span class="nav-text">完善模型功能</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B"><span class="nav-number">4.1.</span> <span class="nav-text">卷积神经网络简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86"><span class="nav-number">4.2.</span> <span class="nav-text">卷积神经网络原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%89%E4%B8%AA%E7%BB%93%E6%9E%84"><span class="nav-number">4.2.1.</span> <span class="nav-text">卷积神经网络三个结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">4.2.2.</span> <span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E8%BE%93%E5%87%BA%E5%80%BC%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">卷积输出值的计算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A1%AB%E5%85%85%E5%92%8C%E5%A4%9AFilter"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">填充和多Filter</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Relu"><span class="nav-number">4.2.3.</span> <span class="nav-text">激活函数-Relu</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E8%AE%A1%E7%AE%97"><span class="nav-number">4.2.4.</span> <span class="nav-text">池化计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="nav-number">4.2.5.</span> <span class="nav-text">全连接层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6"><span class="nav-number">4.3.</span> <span class="nav-text">实例探究</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inception-ResNet-v2%E4%BB%8B%E7%BB%8D"><span class="nav-number">5.</span> <span class="nav-text">Inception-ResNet-v2介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception"><span class="nav-number">5.1.</span> <span class="nav-text">Inception</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet"><span class="nav-number">5.2.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception-ResNet-v2"><span class="nav-number">5.3.</span> <span class="nav-text">Inception-ResNet-v2</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Inception-ResNet-v2%E8%BF%9B%E8%A1%8C%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB"><span class="nav-number">6.</span> <span class="nav-text">使用Inception-ResNet-v2进行垃圾分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D-1"><span class="nav-number">6.1.</span> <span class="nav-text">数据集介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="nav-number">6.2.</span> <span class="nav-text">处理流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A"><span class="nav-number">6.2.1.</span> <span class="nav-text">数据预处理：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.2.2.</span> <span class="nav-text">构造模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="nav-number">6.2.3.</span> <span class="nav-text">模型训练与保存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E4%B8%8E%E9%A2%84%E6%B5%8B"><span class="nav-number">6.2.4.</span> <span class="nav-text">模型加载与预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="nav-number">6.3.</span> <span class="nav-text">结果分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E4%BB%B6"><span class="nav-number">7.</span> <span class="nav-text">附件</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="飞木鱼"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">飞木鱼</p>
  <div class="site-description" itemprop="description">整理一些值得留意的文章</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Z3Zm1pYW8=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fwfmiao"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmxpdS1mZW5neXVAb3V0bG9vay5jb20=" title="E-Mail → mailto:liu-fengyu@outlook.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tLw==" title="https:&#x2F;&#x2F;github.com&#x2F;">Github</span>
        </li>
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cHM6Ly93d3d3LmJpbGliaWxpLmNvbS8=" title="https:&#x2F;&#x2F;wwww.bilibili.com&#x2F;">Bilibili</span>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">飞木鱼</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">92k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:24</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  
  <script data-pjax>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>









<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://lfyfzy.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://liufengyu.cn/posts/trash-classification.html";
    this.page.identifier = "/posts/trash-classification.html";
    this.page.title = "【深度学习】手把手教你用卷积神经网络做垃圾分类";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://lfyfzy.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
</body>
</html>
